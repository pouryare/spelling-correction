{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Set, Tuple\n",
        "from google.colab import drive\n",
        "from joblib import dump, load\n",
        "import graphviz\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "n9Jj5pm4bhjp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqzGPBu2bheC",
        "outputId": "89487a3e-f38f-482a-d8d1-1c2620388e1b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/ML/Correct spelling/papers.csv')"
      ],
      "metadata": {
        "id": "kbQX7UHIbhYe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom stop words\n",
        "custom_stop_words = [\n",
        "    \"fig\", \"figure\", \"image\", \"sample\", \"using\",\n",
        "    \"show\", \"result\", \"large\",\n",
        "    \"also\", \"one\", \"two\", \"three\",\n",
        "    \"four\", \"five\", \"seven\", \"eight\", \"nine\"\n",
        "]"
      ],
      "metadata": {
        "id": "_28bxL0_bhTI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the set of English stop words and add custom stop words\n",
        "stop_words: Set[str] = set(stopwords.words('english')).union(custom_stop_words)\n",
        "\n",
        "def pre_process(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Preprocess the input text by applying various cleaning and normalization techniques.\n",
        "\n",
        "    Args:\n",
        "        text (str): The input text to be preprocessed.\n",
        "\n",
        "    Returns:\n",
        "        str: The preprocessed text.\n",
        "\n",
        "    This function applies the following preprocessing steps:\n",
        "    1. Convert to lowercase\n",
        "    2. Remove HTML tags\n",
        "    3. Remove special characters and digits\n",
        "    4. Tokenize the text\n",
        "    5. Remove stop words\n",
        "    6. Remove short words (less than 3 characters)\n",
        "    7. Lemmatize the words\n",
        "    \"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(\"&lt;/?.*?&gt;\", \" &lt;&gt; \", text)\n",
        "\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r\"(\\d|\\W)+\", \" \", text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stop words and short words\n",
        "    words = [word for word in words if word not in stop_words and len(word) >= 3]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "T7l_dqRSbhNl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to all rows\n",
        "docs = df['paper_text'].apply(pre_process)\n",
        "\n",
        "print(\"Sample of preprocessed text:\")\n",
        "print(docs.iloc[0])\n",
        "print(\"\\nShape of preprocessed data:\", docs.shape)\n",
        "\n",
        "def get_word_count(words: List[str]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Count the occurrences of each word in the input list.\n",
        "\n",
        "    Args:\n",
        "        words (List[str]): A list of words.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: A dictionary where the key is the word and the value is its frequency.\n",
        "    \"\"\"\n",
        "    return Counter(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esq-bU4ubhJQ",
        "outputId": "b755c4bf-1fc2-4f2e-ca13-574a5eec05c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of preprocessed text:\n",
            "self organization associative database application hisashi suzuki suguru arimoto osaka university toyonaka osaka japan abstract efficient method self organizing associative database proposed together application robot eyesight system proposed database associate input output first half part discussion algorithm self organization proposed aspect hardware produce new style neural network latter half part applicability handwritten letter recognition autonomous mobile robot system demonstrated introduction let mapping given finite infinite set another finite infinite set learning machine observes set pair sampled randomly mean cartesian product computes estimate make small estimation error measure usually say faster decrease estimation error increase number sample better learning machine however expression performance incomplete since lack consideration candidate assumed preliminarily find good learning machine clarify conception let discus type learning machine let advance understanding self organization associative database parameter type ordinary type learning machine assumes equation relating parameter indefinite namely structure equivalent define implicitly set candidate subset mapping computes value parameter based observed sample call type parameter type learning machine defined well approach number sample increase alternative case however estimation error remains eternally thus problem designing learning machine return find proper structure sense hand assumed structure demanded compact possible achieve fast learning word number parameter small since parameter uniquely determined even though observed sample however demand proper contradicts compact consequently parameter type better compactness assumed structure proper better learning machine elementary conception design learning machine universality ordinary neural network suppose sufficient knowledge given though unknown case comparatively easy find proper compact structure alternative case however sometimes difficult possible solution give compactness assume almighty structure cover various combination orthogonal base infinite dimension structure neural network approximation obtained truncating finitely dimension implementation american institute physic main topic designing neural network establish desirable structure work includes developing practical procedure compute value coefficient observed sample discussion flourishing since many efficient method proposed recently even hardware unit computing coefficient parallel speed sold anza mark iii odyssey nevertheless neural network always exists danger error remaining eternally estimating precisely speaking suppose combination base finite number define structure essentially word suppose located near case estimation error none negligible however distant estimation error never becomes negligible indeed many research report following situation appears complex estimation error converges value number sample increase decrease hardly even though dimension heighten property sometimes considerable defect neural network recursi type recursive type founded another methodology learning follows initial stage set instead notation candidate equal set mapping observing first reduced observing second reduced thus candidate set becomes gradually small observation sample proceeds observing sample write likelihood estimation selected hence contrarily parameter type recursive type guarantee surely approach number sample increase recursive type observes rewrite value correlated hence type architecture composed rule rewriting free memory space architecture form naturally kind database build management system data self organizing way however database differs ordinary one following sense record sample already observed computes estimation call database associative database first subject constructing associative database establish rule rewri ting purpose adap measure called dissimilari dissimilari mean mapping real whenever however necessarily defined single formula definable example collection rule written form dissimilarity defines structure locally hence even though knowledge imperfect flect heuristic way hence contrarily neural network possible accelerate speed learning establishing well especially easily find simple process analogically information like human see application paper recursive type show strongly effectiveness denote sequence observed sample simplest construction associative database observing sample follows algorithm initial stage let empty set every let equal min furthermore add produce another version improved economize memory follows algorithm initial stage let composed arbitrary element every let lex equal min furthermore let add produce either construction approach increase however computation time grows proportionally size second subject constructing associative database addressing rule employ economize computation time subsequent chapter construction associative database purpose proposed manages data form binary tree self organization associative database given sequence algorithm constructing associative database follows algorithm step initialization let root root variable assigned respective node memorize data furthermore let step increase put reset pointer root repeat following arrives terminal node leaf notation nand let mean descendant node otherwise let step display yin related information next put yin back step otherwise first establish new descendant node secondly let yin yin yin finally back step loop step stopped time continued suppose gate element namely artificial synapsis play role branching prepared obtain new style neural network gate element randomly connected algorithm letter recognition recen tly vertical slitting method recognizing typographic english letter elastic matching method recognizing hand written discrete english letter global training fuzzy logic search method recognizing chinese character written square style etc published self organization associative database realizes recognition handwritten continuous english letter nov source document loo windowing number sample nualber sampl experiment scanner take document letter recognizer us parallelogram window least cover maximal letter process sequence letter shifting window recognizer scan word slant direction place window left vicinity may first black point detected window catch letter part succeeding letter recognition head letter performed end position namely boundary line letter becomes known hence starting scanning boundary repeating operation recognizer accomplishes recursively task thus major problem come identifying head letter window considering define following regard window image define accordingly denote black point left area boundary window project onto window measure euclidean distance black point closest let summation black point divided number regard couple reading position boundary define accordingly operator teach recognizer interaction relation window reading boundary algorithm precisely recalled reading incorrect operator teach correct reading via console moreover boundary position incorrect teach correct position via mouse show partially document used experiment show change number node recognition rate defined relative frequency correct answer past trial speciiications window height dot width dot slant angular deg example level tree distributed time recognition rate converged experimentally recognition rate converges case rare case however attain since distinguishable excessive lluctuation writing consistency relation assured like number node increase endlessly hence clever stop learning recognition rate attains upper limit improve recognition rate must consider spelling word future subject obstacle avoiding movement various system camera type autonomous mobile robot reported flourishingly system made author belongs category mathematical methodology solve usually problem obstacle avoiding movement cost minimization problem cost criterion established artificially contrarily self organization associative database reproduces faithfully cost criterion operator therefore motion robot learning becomes natural length width height robot weight visual angle camera deg robot following factor motion turn le deg advance le control speed le experiment done passageway wid inside building author laboratory exist experimental intention arrange box smoking stand gas cylinder stool handcart etc passage way random let robot take camera recall similar trace route preliminarily recorded purpose define following let camera face deg downward take process low pas filter scanning vertically filtered bottom top search first point luminance change excessively bstitu point bottom white point top black obstacle exists front robot white area show free area robot move around regard binary dot image processed thus define accordingly every let number black point exclusive regard image obtained drawing route image define accordingly robot superimposes current camera route recalled inquires operator instruction operator judge subjectively whether suggested route appropriate negative answer draw desirable route mouse teach new robot opera tion defines implicitly sequence reflecting cost criterion operator iibube roan stationary uni configuration autonomous mobile robot system north rmbi unit robot roan experimental environment wall camera preprocessing preprocessing course suggest ion search processing obstacle avoiding movement processing position identification define satisfaction rate relative frequency acceptable suggestion route past trial typical experiment change satisfaction rate showed similar tendency attains around time notice rest mean directly percentage collision practice prevent collision adopting supplementary measure time number node level tree distributed proposed method reflects delicately various character operator example robot trained operator move slowly enough space obstacle trained another operator brush quickly obstacle fact give hint method printing character machine position identification robot identify position recalling similar landscape position data camera purpose principle suffices regard camera image position data respectively however memory capacity finite actual compu ters hence cannot compress camera image slight loss information compression admittable long precision position identification acceptable area thus major problem come find suitable compression method experimental environment jut passageway interval section adjacent jut door robot identifies roughly surrounding landscape section place us temporarily triangular surveying technique exact measure necessary realize former task define following turn camera take panorama deg scanning horizontally center line substitute point luminance excessively change black point white regard binary dot line image processed thus define accordingly every project black point onto measure euclidean distance black point closest let summation similarly calculate exchanging role denoting number respectively nand define regard positive integer labeled section define accordingly learning mode robot check exactly position counter reset periodically operator robot run arbitrarily passageway within area learns relation landscape position data position identification beyond area achieved crossing plural database another task automatic excepting periodic reset counter namely kind learning without teacher define identification rate relative frequency correct recall position data past trial typical example converged around time time number level level oftree distributed since identification failure rejected considering trajectory pro blem arises practical use order improve identification rate compression ratio camera image must loosened possibility depends improvement hardware future show example actual motion robot based database obstacle avoiding movement position identification example corresponds case moving time interval per frame sec actual motion robot conclusion method self organizing associative database proposed application robot eyesight system machine decomposes global structure unknown set local structure known learns universally input output response framework problem implies wide application area example shown paper defect algorithm self organization tree balanced well subclass structure subject imposed widen class probable solution abolish addressing rule depending directly value instead establish another rule depending distribution function value investigation reference hopfield tank computing neural circuit model science rumelhart learning representation back propagating error nature hull hypothesis generation computational model visual word recognition ieee expert fall kurtzberg feature analysis symbol recognition elastic matching ibm re develop wang suen tree classifier heuristic search global training ieee trans pattern anal mach intell pami brook self calibration motion stereo vision mobile robot int symp robotics research goto stentz cmu system mobile robot navigation ieee int conf robotics automation madarasz design autonomous vehicle disabled ieee jour robotics automation triendl kriegman stereo vision navigation within building ieee int conf robotics automation turk video road following autonomous land vehicle ieee int conf robotics automation\n",
            "\n",
            "Shape of preprocessed data: (7241,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all preprocessed texts and tokenize\n",
        "all_words = ' '.join(docs).split()"
      ],
      "metadata": {
        "id": "Z_hxWedibhFy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get word count\n",
        "word_count_dict = get_word_count(all_words)\n",
        "\n",
        "print(\"\\nSample of word counts:\")\n",
        "print(dict(list(word_count_dict.items())[:5]))\n",
        "\n",
        "def get_probs(word_count_dict: Dict[str, int]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate the probability of each word based on its frequency.\n",
        "\n",
        "    Args:\n",
        "        word_count_dict (Dict[str, int]): A dictionary where the key is the word and the value is its frequency.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary where the key is the word and the value is its probability.\n",
        "    \"\"\"\n",
        "    total_words = sum(word_count_dict.values())\n",
        "    return {word: count / total_words for word, count in word_count_dict.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjxryCOJbhAR",
        "outputId": "011083e3-e19d-4a58-f237-f5c682ad3457"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample of word counts:\n",
            "{'self': 3766, 'organization': 1321, 'associative': 1423, 'database': 3994, 'application': 17552}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate word probabilities\n",
        "probs = get_probs(word_count_dict)\n",
        "\n",
        "print(\"\\nSample of word probabilities:\")\n",
        "print(dict(list(probs.items())[:5]))\n",
        "\n",
        "def delete_letter(word: str, verbose: bool = False) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate all possible strings that have one character deleted from the input word.\n",
        "\n",
        "    Args:\n",
        "        word (str): The input word.\n",
        "        verbose (bool): If True, print detailed information about the process.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of words with one character deleted.\n",
        "    \"\"\"\n",
        "    split_l = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    delete_l = [L + R[1:] for L, R in split_l if R]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
        "\n",
        "    return delete_l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkOivhE8b848",
        "outputId": "c79034d7-1171-430e-aeb3-415912e88857"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample of word probabilities:\n",
            "{'self': 0.00023627943336854102, 'organization': 8.287974813591149e-05, 'associative': 8.927924420696598e-05, 'database': 0.00025058418929207457, 'application': 0.001101215245481846}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def switch_letter(word: str, verbose: bool = False) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate all possible strings that have two adjacent characters switched.\n",
        "\n",
        "    Args:\n",
        "        word (str): The input word.\n",
        "        verbose (bool): If True, print detailed information about the process.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of words with adjacent characters switched.\n",
        "    \"\"\"\n",
        "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
        "    switch_l = [a + b[1] + b[0] + b[2:] for a, b in split_l if len(b) >= 2]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\")\n",
        "\n",
        "    return switch_l"
      ],
      "metadata": {
        "id": "rAAJeoylb8jd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_letter(word: str, verbose: bool = False) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate all possible strings that have one character replaced by another valid character.\n",
        "\n",
        "    Args:\n",
        "        word (str): The input word.\n",
        "        verbose (bool): If True, print detailed information about the process.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of words with one character replaced.\n",
        "    \"\"\"\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
        "    replace_l = [a + l + (b[1:] if len(b) > 1 else '') for a, b in split_l if b for l in letters]\n",
        "    replace_set = set(replace_l) - {word}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l = {sorted(list(replace_set))}\")\n",
        "\n",
        "    return sorted(list(replace_set))"
      ],
      "metadata": {
        "id": "Zjki1uLpb8ek"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_letter(word: str, verbose: bool = False) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate all possible strings that have an extra character inserted.\n",
        "\n",
        "    Args:\n",
        "        word (str): The input word.\n",
        "        verbose (bool): If True, print detailed information about the process.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of words with one character inserted.\n",
        "    \"\"\"\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    split_l = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    insert_l = [a + l + b for a, b in split_l for l in letters]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
        "\n",
        "    return insert_l"
      ],
      "metadata": {
        "id": "xB8KKkVVb8X9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_one_letter(word: str, allow_switches: bool = True) -> Set[str]:\n",
        "    \"\"\"\n",
        "    Generate all strings that are one edit away from the input word.\n",
        "\n",
        "    Args:\n",
        "        word (str): The input word.\n",
        "        allow_switches (bool): If True, include switched-character edits.\n",
        "\n",
        "    Returns:\n",
        "        Set[str]: A set of words that are one edit away from the input word.\n",
        "    \"\"\"\n",
        "    edit_set = set(delete_letter(word))\n",
        "    edit_set.update(replace_letter(word))\n",
        "    edit_set.update(insert_letter(word))\n",
        "    if allow_switches:\n",
        "        edit_set.update(switch_letter(word))\n",
        "    return edit_set"
      ],
      "metadata": {
        "id": "t5lsU1hRcPdw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edit_two_letters(word: str, allow_switches: bool = True) -> Set[str]:\n",
        "    \"\"\"\n",
        "    Generate all strings that are two edits away from the input word.\n",
        "\n",
        "    Args:\n",
        "        word (str): The input word.\n",
        "        allow_switches (bool): If True, include switched-character edits.\n",
        "\n",
        "    Returns:\n",
        "        Set[str]: A set of words that are two edits away from the input word.\n",
        "    \"\"\"\n",
        "    edit_set = set()\n",
        "    edit_one = edit_one_letter(word, allow_switches)\n",
        "    for w in edit_one:\n",
        "        if w:\n",
        "            edit_set.update(edit_one_letter(w, allow_switches))\n",
        "    return edit_set"
      ],
      "metadata": {
        "id": "9hHHaZRNcPRu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_corrections(word: str, probs: Dict[str, float], vocab: Set[str], verbose: bool = False) -> List[Tuple[str, float]]:\n",
        "    \"\"\"\n",
        "    Generate spelling correction suggestions for the input word.\n",
        "\n",
        "    Args:\n",
        "        word (str): The input word to check for suggestions.\n",
        "        probs (Dict[str, float]): A dictionary that maps each word to its probability in the corpus.\n",
        "        vocab (Set[str]): A set containing all the vocabulary.\n",
        "        verbose (bool): If True, print detailed information about the process.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[str, float]]: A list of tuples with suggested corrections and their probabilities.\n",
        "    \"\"\"\n",
        "    suggestions = list(edit_two_letters(word).intersection(vocab))\n",
        "    n_best = [[s, probs.get(s, 0)] for s in suggestions]\n",
        "\n",
        "    if verbose:\n",
        "        print(\"suggestions = \", suggestions)\n",
        "\n",
        "    return n_best"
      ],
      "metadata": {
        "id": "12lCXJS0cPKN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary set\n",
        "vocab = set(word_count_dict.keys())\n",
        "\n",
        "print(\"\\nVocabulary size:\", len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKT9Br79cO6Y",
        "outputId": "21406ab2-244b-4e0d-8e30-75efacf97ce5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary size: 169421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_spelling_correction_flowchart(test_word: str,\n",
        "                                         one_edits: Set[str],\n",
        "                                         two_edits: Set[str],\n",
        "                                         valid_words: Set[str],\n",
        "                                         corrections: List[Tuple[str, float]]):\n",
        "    \"\"\"\n",
        "    Create and display an inline flowchart visualization of the spelling correction process.\n",
        "\n",
        "    Args:\n",
        "        test_word (str): The original misspelled word.\n",
        "        one_edits (Set[str]): Set of words one edit away from the test word.\n",
        "        two_edits (Set[str]): Set of words two edits away from the test word.\n",
        "        valid_words (Set[str]): Set of valid words found in the vocabulary.\n",
        "        corrections (List[Tuple[str, float]]): List of suggested corrections with their probabilities.\n",
        "    \"\"\"\n",
        "    dot = graphviz.Digraph(comment='Spelling Correction Flowchart')\n",
        "    dot.attr(rankdir='TB', size='12,12')\n",
        "\n",
        "    # Define color scheme\n",
        "    colors = {\n",
        "        'start': '#E6F3FF',  # Light blue\n",
        "        'process': '#FFF2CC',  # Light yellow\n",
        "        'decision': '#E2F0D9',  # Light green\n",
        "        'end': '#FCE4D6'  # Light orange\n",
        "    }\n",
        "\n",
        "    # Define node styles\n",
        "    dot.attr('node', shape='rectangle', style='filled', fontname='Arial', fontsize='12')\n",
        "\n",
        "    # Start node\n",
        "    dot.node('A', f'Input word:\\n\"{test_word}\"', fillcolor=colors['start'])\n",
        "\n",
        "    # One-edit words\n",
        "    dot.node('B', f'Generate one-edit words\\n(e.g., {\", \".join(list(one_edits)[:3])}...)', fillcolor=colors['process'])\n",
        "    dot.edge('A', 'B')\n",
        "\n",
        "    # Two-edit words\n",
        "    dot.node('C', f'Generate two-edit words\\n(e.g., {\", \".join(list(two_edits)[:3])}...)', fillcolor=colors['process'])\n",
        "    dot.edge('B', 'C')\n",
        "\n",
        "    # Filter valid words\n",
        "    dot.node('D', f'Filter valid words\\n(e.g., {\", \".join(list(valid_words)[:3])}...)', fillcolor=colors['decision'])\n",
        "    dot.edge('C', 'D')\n",
        "\n",
        "    # Calculate probabilities\n",
        "    dot.node('E', 'Calculate probabilities', fillcolor=colors['process'])\n",
        "    dot.edge('D', 'E')\n",
        "\n",
        "    # Sort by probability\n",
        "    dot.node('F', 'Sort by probability', fillcolor=colors['process'])\n",
        "    dot.edge('E', 'F')\n",
        "\n",
        "    # Top suggestions\n",
        "    suggestions = '\\n'.join([f'{word}: {prob:.6f}' for word, prob in corrections[:5]])\n",
        "    dot.node('G', f'Top suggestions:\\n{suggestions}', fillcolor=colors['end'])\n",
        "    dot.edge('F', 'G')\n",
        "\n",
        "    # Display the flowchart inline\n",
        "    display(dot)"
      ],
      "metadata": {
        "id": "DNIR2U4VcO0H"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the spelling correction\n",
        "test_word = 'algoritm'\n",
        "one_edits = edit_one_letter(test_word)\n",
        "two_edits = edit_two_letters(test_word)\n",
        "valid_words = two_edits.intersection(vocab)\n",
        "word_probs = [(word, probs.get(word, 0)) for word in valid_words]\n",
        "corrections = sorted(word_probs, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(f\"\\nSpelling suggestions for '{test_word}':\")\n",
        "for i, (word, prob) in enumerate(corrections[:10]):  # Show top 10 suggestions\n",
        "    print(f\"Suggestion {i+1}: {word} (probability: {prob:.6f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyTsiqdYkCRt",
        "outputId": "6557090b-cf64-47ec-93f8-262ec991e76e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Spelling suggestions for 'algoritm':\n",
            "Suggestion 1: algorithm (probability: 0.007304)\n",
            "Suggestion 2: lgorithm (probability: 0.000002)\n",
            "Suggestion 3: algorit (probability: 0.000001)\n",
            "Suggestion 4: algori (probability: 0.000000)\n",
            "Suggestion 5: algortihm (probability: 0.000000)\n",
            "Suggestion 6: algoritm (probability: 0.000000)\n",
            "Suggestion 7: algoritms (probability: 0.000000)\n",
            "Suggestion 8: algoirthm (probability: 0.000000)\n",
            "Suggestion 9: palgorithm (probability: 0.000000)\n",
            "Suggestion 10: algorihtms (probability: 0.000000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and display the flowchart\n",
        "create_spelling_correction_flowchart(test_word, one_edits, two_edits, valid_words, corrections)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "6VQksv2PkGW5",
        "outputId": "36ae0c47-55dc-4421-bfa4-98ddcee23bb6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"223pt\" height=\"526pt\"\n viewBox=\"0.00 0.00 223.00 526.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 522)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-522 219,-522 219,4 -4,4\"/>\n<!-- A -->\n<g id=\"node1\" class=\"node\">\n<title>A</title>\n<polygon fill=\"#e6f3ff\" stroke=\"black\" points=\"145,-518 70,-518 70,-482 145,-482 145,-518\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-503.4\" font-family=\"Arial\" font-size=\"12.00\">Input word:</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-490.4\" font-family=\"Arial\" font-size=\"12.00\">&quot;algoritm&quot;</text>\n</g>\n<!-- B -->\n<g id=\"node2\" class=\"node\">\n<title>B</title>\n<polygon fill=\"#fff2cc\" stroke=\"black\" points=\"205,-446 10,-446 10,-410 205,-410 205,-446\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-431.4\" font-family=\"Arial\" font-size=\"12.00\">Generate one&#45;edit words</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-418.4\" font-family=\"Arial\" font-size=\"12.00\">(e.g., qlgoritm, algortitm, algoriti...)</text>\n</g>\n<!-- A&#45;&gt;B -->\n<g id=\"edge1\" class=\"edge\">\n<title>A&#45;&gt;B</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.5,-481.7C107.5,-473.98 107.5,-464.71 107.5,-456.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111,-456.1 107.5,-446.1 104,-456.1 111,-456.1\"/>\n</g>\n<!-- C -->\n<g id=\"node3\" class=\"node\">\n<title>C</title>\n<polygon fill=\"#fff2cc\" stroke=\"black\" points=\"213.5,-374 1.5,-374 1.5,-338 213.5,-338 213.5,-374\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-359.4\" font-family=\"Arial\" font-size=\"12.00\">Generate two&#45;edit words</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-346.4\" font-family=\"Arial\" font-size=\"12.00\">(e.g., aulgkritm, jilgoritm, alworitmk...)</text>\n</g>\n<!-- B&#45;&gt;C -->\n<g id=\"edge2\" class=\"edge\">\n<title>B&#45;&gt;C</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.5,-409.7C107.5,-401.98 107.5,-392.71 107.5,-384.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111,-384.1 107.5,-374.1 104,-384.1 111,-384.1\"/>\n</g>\n<!-- D -->\n<g id=\"node4\" class=\"node\">\n<title>D</title>\n<polygon fill=\"#e2f0d9\" stroke=\"black\" points=\"215,-302 0,-302 0,-266 215,-266 215,-302\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-287.4\" font-family=\"Arial\" font-size=\"12.00\">Filter valid words</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-274.4\" font-family=\"Arial\" font-size=\"12.00\">(e.g., algorithma, algoithm, algoritm...)</text>\n</g>\n<!-- C&#45;&gt;D -->\n<g id=\"edge3\" class=\"edge\">\n<title>C&#45;&gt;D</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.5,-337.7C107.5,-329.98 107.5,-320.71 107.5,-312.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111,-312.1 107.5,-302.1 104,-312.1 111,-312.1\"/>\n</g>\n<!-- E -->\n<g id=\"node5\" class=\"node\">\n<title>E</title>\n<polygon fill=\"#fff2cc\" stroke=\"black\" points=\"175.5,-230 39.5,-230 39.5,-194 175.5,-194 175.5,-230\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-208.9\" font-family=\"Arial\" font-size=\"12.00\">Calculate probabilities</text>\n</g>\n<!-- D&#45;&gt;E -->\n<g id=\"edge4\" class=\"edge\">\n<title>D&#45;&gt;E</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.5,-265.7C107.5,-257.98 107.5,-248.71 107.5,-240.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111,-240.1 107.5,-230.1 104,-240.1 111,-240.1\"/>\n</g>\n<!-- F -->\n<g id=\"node6\" class=\"node\">\n<title>F</title>\n<polygon fill=\"#fff2cc\" stroke=\"black\" points=\"164,-158 51,-158 51,-122 164,-122 164,-158\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-136.9\" font-family=\"Arial\" font-size=\"12.00\">Sort by probability</text>\n</g>\n<!-- E&#45;&gt;F -->\n<g id=\"edge5\" class=\"edge\">\n<title>E&#45;&gt;F</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.5,-193.7C107.5,-185.98 107.5,-176.71 107.5,-168.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111,-168.1 107.5,-158.1 104,-168.1 111,-168.1\"/>\n</g>\n<!-- G -->\n<g id=\"node7\" class=\"node\">\n<title>G</title>\n<polygon fill=\"#fce4d6\" stroke=\"black\" points=\"168.5,-86 46.5,-86 46.5,0 168.5,0 168.5,-86\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-72.4\" font-family=\"Arial\" font-size=\"12.00\">Top suggestions:</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-59.4\" font-family=\"Arial\" font-size=\"12.00\">algorithm: 0.007304</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-46.4\" font-family=\"Arial\" font-size=\"12.00\">lgorithm: 0.000002</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-33.4\" font-family=\"Arial\" font-size=\"12.00\">algorit: 0.000001</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-20.4\" font-family=\"Arial\" font-size=\"12.00\">algori: 0.000000</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-7.4\" font-family=\"Arial\" font-size=\"12.00\">algortihm: 0.000000</text>\n</g>\n<!-- F&#45;&gt;G -->\n<g id=\"edge6\" class=\"edge\">\n<title>F&#45;&gt;G</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.5,-121.58C107.5,-114.34 107.5,-105.51 107.5,-96.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"111,-96.18 107.5,-86.18 104,-96.18 111,-96.18\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7b4676967070>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the word probabilities and vocabulary using joblib\n",
        "dump(probs, '/content/drive/MyDrive/ML/Correct spelling/word-probability-spellings.joblib')\n",
        "dump(vocab, '/content/drive/MyDrive/ML/Correct spelling/vocab-spellings.joblib')\n",
        "\n",
        "print(\"\\nWord probabilities and vocabulary saved successfully using joblib.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpkACt_gcOoW",
        "outputId": "35f001ff-9c02-44f2-9daf-8da1cd3e9415"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word probabilities and vocabulary saved successfully using joblib.\n"
          ]
        }
      ]
    }
  ]
}